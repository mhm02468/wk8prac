---
title: "Prac8"
output: html_document
date: "2022-12-01"
---

## Loading Libraries; Downloading, Cleaning and Merging Data 

KEY: YOU HAVE TO CHECK NORMAL (transform if not), MULTICOLINIARITY, CHECK ERRORS (performance package's check model) - map the residuals!

```{r}
library(tidyverse)
library(tmap)
library(geojsonio)
library(plotly)
library(rgdal)
library(broom)
library(mapview)
library(crosstalk)
library(sf)
library(sp)
library(spdep)
library(car)
library(fs)
library(janitor)
library(broom)
tidy(model1)
library(tidypredict)
#library(tidymodels) = there is an issue with this package
library(ggplot2)
library(dplyr)
library(corrr)
library(performance)
library(spatialreg)
library(lmtest)

download.file("https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip", destfile="prac8_data/statistical-gis-boundaries-london.zip")

listfiles <- dir_info(here::here("prac8_data")) %>% 
  dplyr::filter(str_detect(path,".zip")) %>% 
  dplyr::select(path) %>% 
  pull() %>% 
  print() %>% 
  as.character() %>% 
  utils::unzip(exdir=here::here("prac8_data"))

Londonwards <- fs::dir_info(here::here("prac8_data", "statistical-gis-boundaries-london", "ESRI")) %>% 
  #$ means exact match
  dplyr::filter(str_detect(path,"London_Ward_CityMerged.shp$")) %>% 
  dplyr::select(path) %>% 
  dplyr::pull() %>% 
  sf::st_read()

LondonWardProfiles <- read_csv("https://data.london.gov.uk/download/ward-profiles-and-atlas/772d2d64-e8c6-46cb-86f9-e52b4c7851bc/ward-profiles-excel-version.csv",na = c("", "NA", "n/a"), col_names = TRUE,  locale = locale(encoding = 'Latin1'))

Datatypelist <- LondonWardProfiles %>% 
  summarise_all(class) %>% 
  pivot_longer(everything(), names_to="All_variables", values_to="Variable_class")

#merging london shapefile of wards and data
LondonWardProfiles <- Londonwards %>% 
  left_join(.,LondonWardProfiles, by = c("GSS_CODE" = "New code"))

#map to check
tmap_mode("plot")
qtm(LondonWardProfiles, fill = "Average GCSE capped point scores - 2014", borders = NULL, fill.palette = "Blues")

#london school location data

london_schools <- read_csv("https://data.london.gov.uk/download/london-schools-atlas/57046151-39a0-45d9-8dc0-27ea7fd02de8/all_schools_xy_2016.csv")

#make the csv into a spatial object
lon_schools_sf <- st_as_sf(london_schools, coords = c("x", "y"), crs = 4326)

#select just secondary schools because that is when you take the GCSEs
lond_sec_schools_sf <- lon_schools_sf %>% 
  filter(PHASE == "Secondary")

tmap_mode("plot")
qtm(lond_sec_schools_sf)
```

## Linear Regression Overview
1. Think of a research Q: What are the factors that might lead to variation in Average GCSE point scores across the city? (Null hypothesis = there is no relationship). This practical will look at absence from school.

**2. For linear regression, you need:**
    **(1) There is a linear relationship between the dependent and independent variables**
    **(2) Residuals/errors in your model should be normally distributed. Check via hist and then transform if necessary**
    **(3) No Multicolinearity in the independent variables**
    **(4) Homoscedasticity:errors/residuals in the model exhibit constant / homogenous variance ** => if you have hetero then you probably have spatial autocorellation)
    **(5) Independence of errors **

3. There are notes on a few packages that are useful for regression in the code below

4. We can also double check that our fit is alright using bootstrap resampling (not in this prac but useful)

5. How do we select variables? Well, you can use logic. BUT, there are also tests (that are not covered in this module). The tests are: subset regression, k-fold cross validation or gradient descent.

6. Other regression methods such as Ridge, LASSO and elastic net regression can reduce the influence of variables that are not useful in the model. But again, this is beyond the scope of this module.

```{r}
#make a scatter plot
q <- qplot(x = `Unauthorised Absence in All Schools (%) - 2013`, y = `Average GCSE capped point scores - 2014`, data=LondonWardProfiles)

q + geom_jitter() + stat_smooth(method="lm", se=FALSE, size=1) #stat_smooth comes from ggplot2

#make the regression model
Regressiondata <- LondonWardProfiles %>% 
  janitor::clean_names() %>% 
  dplyr::select(average_gcse_capped_point_scores_2014, unauthorised_absence_in_all_schools_percent_2013)

model1 <- Regressiondata %>% 
  lm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013, data=.)

summary(model1)

#tiding the output using the broom package and we can use the tidypredict package to see predictions for individual points. The latter adds a fit column to the code. The tidymodels package does something similar BUT it allows us to change our models more easily should we choose to do so.
glance(model1)

Regressiondata %>% tidypredict_to_column(model1)

#This is the tidymodel package, but the library doesn't seem to load for me, so will come back to this
#lm_mod <- linear_reg() #set the model
#lm_fit <- lm_mod %>% #fit the model
#  fit(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013, data=Regressiondata)
#tidy(lm_fit)
```
### Assumption 1: There is a linear relationship between the dependent and independent variables
1.) If there is a nonlinear relationship, you need to transform the data. Doing so via Tukey’s ladder of transformations works well. So, you do (1) hist (2) tukey's ladder (3) change the power that the plot tells you

```{r}
#check the distribution of data for the variables
ggplot(LondonWardProfiles, aes(x=`Average GCSE capped point scores - 2014`))+ geom_histogram(aes(y = ..density..), binwidth = 5)+geom_density(colour="purple", size=1,adjust=1) # the  ..density.. means the value of a histogram is a density plot

ggplot(LondonWardProfiles, aes(x=`Unauthorised Absence in All Schools (%) - 2013`)) + geom_histogram(aes(y = ..density..), binwidth = 0.1)+geom_density(colour="purple", size=1,adjust=1)

#comparing with median house prices (he changed name of column), which is quite different and needs to be transformed
#for some reason this is not working... will need to address (the error is could not find function "%>%")

LondonWardProfiles <- LondonWardProfiles %>% 
  dplyr::rename(median_house_price_2014 =`Median House Price (£) - 2014`) %>% 
  janitor::clean_names() 

ggplot(LondonWardProfiles, aes(x=median_house_price_2014)) + geom_histogram()

qplot(x = median_house_price_2014, y = average_gcse_capped_point_scores_2014, data=LondonWardProfiles)

#Working through transformation

ggplot(LondonWardProfiles, aes(x=log(median_house_price_2014))) + geom_histogram()

symbox(~median_house_price_2014, LondonWardProfiles, na.rm=T, powers=seq(-3,3, by=.5))
#where the boxplot seems normal will tell us what power to use when we transform our dataset

#transform by raising to the power of -1
ggplot(LondonWardProfiles, aes(x=log(median_house_price_2014)^-1))+geom_histogram()

#Also, check via scatter plot
qplot(x = (median_house_price_2014)^-1, y = average_gcse_capped_point_scores_2014, data=LondonWardProfiles)

```

### Assumption 2: Residuals/errors in your model should be normally distributed
1.) plot the residuals on a hist; if not normally dist


##Q: What do we do if the residuals are not normally dist? Do we just not run the regression? Do we transform? Do we change variables? If we transform, how does that affect the rest of the code and process?

```{r}

#save the residuals
model_data <- model1 %>% 
  augment(., Regressiondata)

#plot
model_data%>%
dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ geom_histogram() 
```

### Assumption 3: No Multicolinearity in the independent variables
1.) Used with multiple regression, this checks that two variables aren't over impacting the model/double counting. To do this, we use the "product moment correlation coefficient between the variables" from thecorrr() pacakge that is part of tidymodels.

2.) We can also use VIF to check for multicolinearity

## Q: why do we remove the residuals and add them to shape layer? Also, how do you decide position = c(10:74)

```{r}
#adding in more variables
Regressiondata2 <- LondonWardProfiles %>% 
  clean_names() %>% 
  dplyr::select(average_gcse_capped_point_scores_2014, unauthorised_absence_in_all_schools_percent_2013, median_house_price_2014)

model2 <- lm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014), data = Regressiondata2)

#show the summary of those outputs
tidy(model2)
glance(model2)

#remove the residuals and add them to shape layer 
model_data2 <- model2 %>%
  augment(., Regressiondata2)

LondonWardProfiles <- LondonWardProfiles %>%
  mutate(model2resids = residuals(model2))

#Check for multicolinearity
Correlation <- LondonWardProfiles %>% 
  st_drop_geometry() %>% 
  dplyr::select(average_gcse_capped_point_scores_2014, unauthorised_absence_in_all_schools_percent_2013, median_house_price_2014) %>%
  mutate(median_house_price_2014=log(median_house_price_2014)) %>% 
  correlate() %>% 
  focus(-average_gcse_capped_point_scores_2014, mirror = TRUE)

rplot(Correlation) #visualise the correlation matrix

#VIF
vif(model2)

#Correlation Matrix for all of the variables
position = c(10:74)
Correlation_all<- LondonWardProfiles %>%
  st_drop_geometry()%>%
  dplyr::select(position)%>%
    correlate()

rplot(Correlation_all)

```

### Assumption 4: Homoscedasticity:errors/residuals in the model exhibit constant / homogenous variance
1.) Why is this important? If your errors do not have constant variance, then your parameter estimates could be wrong, as could the estimates of their significance.
2.) There are two ways to code plots to address this

Here is what to look for in the plots:

Residuals vs Fitted: a flat and horizontal line. This is looking at the linear relationship assumption between our variables

Normal Q-Q: all points falling on the line. This checks if the residuals (observed minus predicted) are normally distributed

Scale vs Location: flat and horizontal line, with randomly spaced points. This is the homoscedasticity (errors/residuals in the model exhibit constant / homogeneous variance). Are the residuals (also called errors) spread equally along all of the data.

Residuals vs Leverage - Identifies outliers (or influential observations), the three largest outliers are identified with values in the plot.

```{r}
#Way 1 to plot to test for this criterion
par(mfrow=c(2,2))
plot(model2)

#Way 2 to plot to test for this criterion, using the performance package
library(performance)
library(see)
check_model(model2, check="all")
```



### Assumption 5: Independence of Errors
1.) residual values (errors) must not be correlated. If they do, then they're autocorrelated which means something hasn't been accounted for in the model
2.) For non-spatial data, we can use the *Durbin Watson test statistic*
3.) FOR SPATIAL DATA
      (1) Map the errors because you can visually get a sense (sometimes) if there are relationships btw the errors (i.e., contiguous polygons have similar values)
      (2) Use Moran's I to map the autocorrelation


**Q: How do we read the DW number? What do high/low values mean?**
```{r}
#check the DW
DW <- durbinWatsonTest(model2)
tidy(DW)

#plot the residuals
tmap_mode("view")
tm_shape(LondonWardProfiles) + tm_polygons("model2resids", palette="RdYlBu") + tm_shape(lond_sec_schools_sf) + tm_dots(col="TYPE")

#adjust for spatial autocorrelation: calculate centroids, create spatial weight matrix,

coordsW <- LondonWardProfiles%>%
  st_centroid()%>%
  st_geometry()

#this is a simple binary matrix of queens cas
LWard_nb <- LondonWardProfiles %>%
  poly2nb(., queen=T)

#this is k nearest neighbours
knn_wards <-coordsW %>%
  knearneigh(., k=4)

LWard_knn <- knn_wards %>% 
  knn2nb()

#create a spatial weights matrix object from these weights, here "W" = row standardising
Lward.queens_weight <- LWard_nb %>%
  nb2listw(., style="W")

Lward.knn_4_weight <- LWard_knn %>%
  nb2listw(., style="W")


#Run the Moran's I
Queen <- LondonWardProfiles %>% 
  st_drop_geometry() %>% 
  dplyr::select(model2resids) %>% 
  pull() %>% 
  moran.test(., Lward.queens_weight) %>% 
  tidy()

Nearest_neighbour <- LondonWardProfiles %>% 
  st_drop_geometry() %>% 
  dplyr::select(model2resids) %>% 
  pull() %>% 
  moran.test(., Lward.knn_4_weight) %>% 
  tidy()

Queen
Nearest_neighbour

# we can see that the Moran’s I statistic is somewhere between 0.27 and 0.29. Remembering that Moran’s I ranges from between -1 and +1 (0 indicating no spatial autocorrelation) we can conclude that there is some weak to moderate spatial autocorrelation in our residuals. This means that despite passing most of the assumptions of linear regression, we could have a situation here where the presence of some spatial autocorrelation could be leading to biased estimates of our parameters and significance values.

```

## Spatial Lag (lagged dependent variable)

#### Key Terms
**Rho** = the *spatial lag* that measures the variable in the surrounding spatial areas as defined by the spatial weight matrix. We use this as an extra explanatory variable to account for clustering (identified by Moran’s I)
**Likelihood ratio (LR) test** *shows if the addition of the lag is an improvement and if that’s significant*. This code would give the same output… THIS IS BASICALLY MORAN'S I
**Lagrange Multiplier (LM)** tests for the absence of spatial autocorrelation in the lag model's residuals. If it is significant you can reject the null hypothesis (no spatial autocorrelation) and accept the alternative (there *is* spatial autocorrelation)
**Wald Test** tests if new parametres (the lag) should be included int he model. If the result is significant than it does need to be included. Very similar to the Lagrange test.
 st_distance(points from and points to)
**SIGNIFICANCE OF THESE IS THE P VALUE, use tidy()**

#### Key Ideas
1. This essentially addresses when Moran's I shows spatial autocorrelation and one spatial unit might be affected by another (there may be lag). In this specific instance, the model over-predicted GCSE scores (those shown in blue in the map above with negative residuals) and under-predicted (those shown in red/orange) occasionally were near to each other. Overlaying the schools suggests this might be impacted by students in a different ward going across a ward boundry to a different school and therefore impacting its scores. SO, the GCSE scores might actually be related to another spatial unit/ward.
2. The solutions is to **incorporate a spatially-lagged version of this variable amongst the independent variables on the right-hand side of the equation.**
3. In the equation: PWiYi means if neighbours have higher values they might have more affect on others (aka: average value for GCSE scores is expected to be higher if, on average, Wards neighbouring also have higher GCSE score values), and here the **p represents spatial lag**
4. **you run a spatial lag regression model using a specific weight matrix**
  Using the queens case (see the code below)
  Using K nearest neighbours (see code below)
5. Note on coefficients:  model is not consistent bc observations change based on the weight matrix neighbours selected which might vary (almost certainly in a distance based matrix) SO we don't interpret the results exactly in the same way as with normal linear regression coefficients. **The only way to do this is through calculating the impact of the spatial lag!** which is impacts(model)
6. In order to compare the coefficients of the spatial lag model, you need to calculate the **"impact of the lag"** (code below) that can be determined in a variety of ways. There are direct and indirect components of this.





#Q: why are you using a global standardisation instead of a row standardisation in slag_dv_model2_queen? **You can use "W" which is row, the numbers will probs just be bigger**
#Q: can I just start with the lagrange multiplier test to tell if there is spatial autocorrelation? What does it mean for lm test and the wald test results to "be significant"? **Significance based on P value for the coefficient**

NEW Q: What is the "direct" and "indirect"?

```{r}
model2 <- lm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014), data =LondonWardProfiles)

#Queens Case
slag_dv_model2_queen <- lagsarlm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014), data =LondonWardProfiles, nb2listw(LWard_nb, style="C"), method = "eigen") #I CAN USE ROW

t<-summary(slag_dv_model2_queen)

lrtest(slag_dv_model2_queen, model2)

#what do the outputs show?
tidy(slag_dv_model2_queen)
#glance(slag_dv_model2_queen)

#calculate the impact of the spatial lag
weight_list <- nb2listw(LWard_knn, style="C")
imp <- impacts(slag_dv_model2_queen, listw=weight_list)

##BEYOND THE SCOPE OF THE MODULE/WHAT WE REALLY NEED TO KNOW. This is used if you have a large data set and wish to do compute the direct and indirect.

#calculating the lag impact by row, "W"
slag_dv_model2_queen_row <- lagsarlm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 +log(median_house_price_2014), data = LondonWardProfiles, nb2listw(LWard_nb, style="W"), method = "eigen")

#This uses a sparse matrix. It takes two arguments: mult and MC
W <- as(weight_list, "CsparseMatrix")
trMatc <- trW(W, type="mult")
trMC <- trW(W, type="MC")
imp2 <- impacts(slag_dv_model2_queen_row, tr=trMatc, R=200)
imp3 <- impacts(slag_dv_model2_queen_row, tr=trMC, R=200)
sum <- summary(imp2,  zstats=TRUE, short=TRUE)

#K Nearest Neighbours
slag_dv_model2_knn4 <- lagsarlm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014), data = LondonWardProfiles, nb2listw(LWard_knn,style="C"),method = "eigen")

tidy(slag_dv_model2_knn4)

```

## Spatial Error

```{r}

```

## Geographically Weighted Regression

```{r}

```



