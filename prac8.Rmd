---
title: "Prac8"
output:
  pdf_document: default
  html_document: default
date: "2022-12-01"
---

## Loading Libraries; Downloading, Cleaning and Merging Data 

KEY: YOU HAVE TO CHECK NORMAL (transform if not), MULTICOLINIARITY, CHECK ERRORS (performance package's check model) - map the residuals!

```{r}
library(tidyverse)
library(tmap)
library(geojsonio)
library(plotly)
library(rgdal)
library(broom)
library(mapview)
library(crosstalk)
library(sf)
library(sp)
library(spdep)
library(car)
library(fs)
library(janitor)
library(broom)
library(tidypredict)
#library(tidymodels) = there is an issue with this package
library(ggplot2)
library(dplyr)
library(corrr)
library(performance)
library(spatialreg)
library(lmtest)
library(spdep)
library(spgwr)

download.file("https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip", destfile="prac8_data/statistical-gis-boundaries-london.zip")

listfiles <- dir_info(here::here("prac8_data")) %>% 
  dplyr::filter(str_detect(path,".zip")) %>% 
  dplyr::select(path) %>% 
  pull() %>% 
  print() %>% 
  as.character() %>% 
  utils::unzip(exdir=here::here("prac8_data"))

Londonwards <- fs::dir_info(here::here("prac8_data", "statistical-gis-boundaries-london", "ESRI")) %>% 
  #$ means exact match
  dplyr::filter(str_detect(path,"London_Ward_CityMerged.shp$")) %>% 
  dplyr::select(path) %>% 
  dplyr::pull() %>% 
  sf::st_read()

LondonWardProfiles <- read_csv("https://data.london.gov.uk/download/ward-profiles-and-atlas/772d2d64-e8c6-46cb-86f9-e52b4c7851bc/ward-profiles-excel-version.csv",na = c("", "NA", "n/a"), col_names = TRUE,  locale = locale(encoding = 'Latin1'))

Datatypelist <- LondonWardProfiles %>% 
  summarise_all(class) %>% 
  pivot_longer(everything(), names_to="All_variables", values_to="Variable_class")

#merging london shapefile of wards and data
LondonWardProfiles <- Londonwards %>% 
  left_join(.,LondonWardProfiles, by = c("GSS_CODE" = "New code"))

#map to check
tmap_mode("plot")
qtm(LondonWardProfiles, fill = "Average GCSE capped point scores - 2014", borders = NULL, fill.palette = "Blues")

#london school location data

london_schools <- read_csv("https://data.london.gov.uk/download/london-schools-atlas/57046151-39a0-45d9-8dc0-27ea7fd02de8/all_schools_xy_2016.csv")

#make the csv into a spatial object
lon_schools_sf <- st_as_sf(london_schools, coords = c("x", "y"), crs = 4326)

#select just secondary schools because that is when you take the GCSEs
lond_sec_schools_sf <- lon_schools_sf %>% 
  filter(PHASE == "Secondary")

tmap_mode("plot")
qtm(lond_sec_schools_sf)
```

## Linear Regression Overview
1. Think of a research Q: What are the factors that might lead to variation in Average GCSE point scores across the city? (Null hypothesis = there is no relationship). This practical will look at absence from school.

**2. For linear regression, you need:**
    **(1) There is a linear relationship between the dependent and independent variables**
    **(2) Residuals/errors in your model should be normally distributed. Check via hist and then transform if necessary**
    **(3) No Multicolinearity in the independent variables**
    **(4) Homoscedasticity:errors/residuals in the model exhibit constant / homogenous variance ** => if you have hetero then you probably have spatial autocorellation)
    **(5) Independence of errors **

3. There are notes on a few packages that are useful for regression in the code below

4. We can also double check that our fit is alright using bootstrap resampling (not in this prac but useful)

5. How do we select variables? Well, you can use logic. BUT, there are also tests (that are not covered in this module). The tests are: subset regression, k-fold cross validation or gradient descent.

6. Other regression methods such as Ridge, LASSO and elastic net regression can reduce the influence of variables that are not useful in the model. But again, this is beyond the scope of this module.

```{r}
#make a scatter plot
q <- qplot(x = `Unauthorised Absence in All Schools (%) - 2013`, y = `Average GCSE capped point scores - 2014`, data=LondonWardProfiles)

q + geom_jitter() + stat_smooth(method="lm", se=FALSE, size=1) #stat_smooth comes from ggplot2

#make the regression model
Regressiondata <- LondonWardProfiles %>% 
  janitor::clean_names() %>% 
  dplyr::select(average_gcse_capped_point_scores_2014, unauthorised_absence_in_all_schools_percent_2013)

model1 <- Regressiondata %>% 
  lm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013, data=.)

summary(model1)

#tiding the output using the broom package and we can use the tidypredict package to see predictions for individual points. The latter adds a fit column to the code. The tidymodels package does something similar BUT it allows us to change our models more easily should we choose to do so.
glance(model1)

Regressiondata %>% tidypredict_to_column(model1)

#This is the tidymodel package, but the library doesn't seem to load for me, so will come back to this
#lm_mod <- linear_reg() #set the model
#lm_fit <- lm_mod %>% #fit the model
#  fit(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013, data=Regressiondata)
#tidy(lm_fit)
```
### Assumption 1: There is a linear relationship between the dependent and independent variables
1.) If there is a nonlinear relationship, you need to transform the data. Doing so via Tukey’s ladder of transformations works well. So, you do (1) hist (2) tukey's ladder (3) change the power that the plot tells you

```{r}
#check the distribution of data for the variables
ggplot(LondonWardProfiles, aes(x=`Average GCSE capped point scores - 2014`))+ geom_histogram(aes(y = ..density..), binwidth = 5)+geom_density(colour="purple", size=1,adjust=1) # the  ..density.. means the value of a histogram is a density plot

ggplot(LondonWardProfiles, aes(x=`Unauthorised Absence in All Schools (%) - 2013`)) + geom_histogram(aes(y = ..density..), binwidth = 0.1)+geom_density(colour="purple", size=1,adjust=1)

#comparing with median house prices (he changed name of column), which is quite different and needs to be transformed
#for some reason this is not working... will need to address (the error is could not find function "%>%")

LondonWardProfiles <- LondonWardProfiles %>% 
  dplyr::rename(median_house_price_2014 =`Median House Price (£) - 2014`) %>% 
  janitor::clean_names() 

ggplot(LondonWardProfiles, aes(x=median_house_price_2014)) + geom_histogram()

qplot(x = median_house_price_2014, y = average_gcse_capped_point_scores_2014, data=LondonWardProfiles)

#Working through transformation

ggplot(LondonWardProfiles, aes(x=log(median_house_price_2014))) + geom_histogram()

symbox(~median_house_price_2014, LondonWardProfiles, na.rm=T, powers=seq(-3,3, by=.5))
#where the boxplot seems normal will tell us what power to use when we transform our dataset

#transform by raising to the power of -1
ggplot(LondonWardProfiles, aes(x=log(median_house_price_2014)^-1))+geom_histogram()

#Also, check via scatter plot
qplot(x = (median_house_price_2014)^-1, y = average_gcse_capped_point_scores_2014, data=LondonWardProfiles)

```

### Assumption 2: Residuals/errors in your model should be normally distributed
1.) plot the residuals on a hist; if not normally dist


##Q: What do we do if the residuals are not normally dist? Do we just not run the regression? Do we transform? Do we change variables? If we transform, how does that affect the rest of the code and process?

```{r}

#save the residuals
model_data <- model1 %>% 
  augment(., Regressiondata)

#plot
model_data%>%
dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ geom_histogram() 
```

### Assumption 3: No Multicolinearity in the independent variables
1.) Used with multiple regression, this checks that two variables aren't over impacting the model/double counting. To do this, we use the "product moment correlation coefficient between the variables" from thecorrr() pacakge that is part of tidymodels.

2.) We can also use VIF to check for multicolinearity

## Q: why do we remove the residuals and add them to shape layer? Also, how do you decide position = c(10:74)

```{r}
#adding in more variables
Regressiondata2 <- LondonWardProfiles %>% 
  clean_names() %>% 
  dplyr::select(average_gcse_capped_point_scores_2014, unauthorised_absence_in_all_schools_percent_2013, median_house_price_2014)

model2 <- lm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014), data = Regressiondata2)

#show the summary of those outputs
tidy(model2)
glance(model2)

#remove the residuals and add them to shape layer 
model_data2 <- model2 %>%
  augment(., Regressiondata2)

LondonWardProfiles <- LondonWardProfiles %>%
  mutate(model2resids = residuals(model2))

#Check for multicolinearity
Correlation <- LondonWardProfiles %>% 
  st_drop_geometry() %>% 
  dplyr::select(average_gcse_capped_point_scores_2014, unauthorised_absence_in_all_schools_percent_2013, median_house_price_2014) %>%
  mutate(median_house_price_2014=log(median_house_price_2014)) %>% 
  correlate() %>% 
  focus(-average_gcse_capped_point_scores_2014, mirror = TRUE)

rplot(Correlation) #visualise the correlation matrix

#VIF
vif(model2)

#Correlation Matrix for all of the variables
position = c(10:74)
Correlation_all<- LondonWardProfiles %>%
  st_drop_geometry()%>%
  dplyr::select(position)%>%
    correlate()

rplot(Correlation_all)

```

### Assumption 4: Homoscedasticity:errors/residuals in the model exhibit constant / homogenous variance
1.) Why is this important? If your errors do not have constant variance, then your parameter estimates could be wrong, as could the estimates of their significance.
2.) There are two ways to code plots to address this

Here is what to look for in the plots:

Residuals vs Fitted: a flat and horizontal line. This is looking at the linear relationship assumption between our variables

Normal Q-Q: all points falling on the line. This checks if the residuals (observed minus predicted) are normally distributed

Scale vs Location: flat and horizontal line, with randomly spaced points. This is the homoscedasticity (errors/residuals in the model exhibit constant / homogeneous variance). Are the residuals (also called errors) spread equally along all of the data.

Residuals vs Leverage - Identifies outliers (or influential observations), the three largest outliers are identified with values in the plot.

```{r}
#Way 1 to plot to test for this criterion
par(mfrow=c(2,2))
plot(model2)

#Way 2 to plot to test for this criterion, using the performance package
library(performance)
library(see)
check_model(model2, check="all")
```



### Assumption 5: Independence of Errors
1.) residual values (errors) must not be correlated. If they do, then they're autocorrelated which means something hasn't been accounted for in the model
2.) For non-spatial data, we can use the *Durbin Watson test statistic*
3.) FOR SPATIAL DATA
      (1) Map the errors because you can visually get a sense (sometimes) if there are relationships btw the errors (i.e., contiguous polygons have similar values)
      (2) Use Moran's I to map the autocorrelation


**Q: How do we read the DW number? What do high/low values mean?**
```{r}
#check the DW
DW <- durbinWatsonTest(model2)
tidy(DW)

#plot the residuals
tmap_mode("view")
tm_shape(LondonWardProfiles) + tm_polygons("model2resids", palette="RdYlBu") + tm_shape(lond_sec_schools_sf) + tm_dots(col="TYPE")

#adjust for spatial autocorrelation: calculate centroids, create spatial weight matrix,

coordsW <- LondonWardProfiles%>%
  st_centroid()%>%
  st_geometry()

#this is a simple binary matrix of queens cas
LWard_nb <- LondonWardProfiles %>%
  poly2nb(., queen=T)

#this is k nearest neighbours
knn_wards <-coordsW %>%
  knearneigh(., k=4)

LWard_knn <- knn_wards %>% 
  knn2nb()

#create a spatial weights matrix object from these weights, here "W" = row standardising
Lward.queens_weight <- LWard_nb %>%
  nb2listw(., style="W")

Lward.knn_4_weight <- LWard_knn %>%
  nb2listw(., style="W")


#Run the Moran's I
Queen <- LondonWardProfiles %>% 
  st_drop_geometry() %>% 
  dplyr::select(model2resids) %>% 
  pull() %>% 
  moran.test(., Lward.queens_weight) %>% 
  tidy()

Nearest_neighbour <- LondonWardProfiles %>% 
  st_drop_geometry() %>% 
  dplyr::select(model2resids) %>% 
  pull() %>% 
  moran.test(., Lward.knn_4_weight) %>% 
  tidy()

Queen
Nearest_neighbour

# we can see that the Moran’s I statistic is somewhere between 0.27 and 0.29. Remembering that Moran’s I ranges from between -1 and +1 (0 indicating no spatial autocorrelation) we can conclude that there is some weak to moderate spatial autocorrelation in our residuals. This means that despite passing most of the assumptions of linear regression, we could have a situation here where the presence of some spatial autocorrelation could be leading to biased estimates of our parameters and significance values.

```

## Spatial Lag (lagged dependent variable)

#### Main Use
The lag model accounts for situations where the value of the dependent variable in one area might be associated with or influenced by the values of that variable in neighbouring zones (however we choose to define neighbouring in our spatial weights matrix). In simplest terms, this essentially addresses when Moran's I shows spatial autocorrelation and one spatial unit might be affected by another (there may be lag).

#### Key Terms
**Rho** = the *spatial lag* that measures the variable in the surrounding spatial areas as defined by the spatial weight matrix. We use this as an extra explanatory variable to account for clustering (identified by Moran’s I)
**Likelihood ratio (LR) test** *shows if the addition of the lag is an improvement and if that’s significant*. This code would give the same output… THIS IS BASICALLY MORAN'S I
**Lagrange Multiplier (LM)** tests for the absence of spatial autocorrelation in the lag model's residuals. If it is significant you can reject the null hypothesis (no spatial autocorrelation) and accept the alternative (there *is* spatial autocorrelation)
**Wald Test** tests if new parametres (the lag) should be included int he model. If the result is significant than it does need to be included. Very similar to the Lagrange test.
 st_distance(points from and points to)
**SIGNIFICANCE OF THESE IS THE P VALUE, use tidy()**

#### Other Noteworthy Ideas
1. This essentially addresses when Moran's I shows spatial autocorrelation and one spatial unit might be affected by another (there may be lag). In this specific instance, the model over-predicted GCSE scores (those shown in blue in the map above with negative residuals) and under-predicted (those shown in red/orange) occasionally were near to each other. Overlaying the schools suggests this might be impacted by students in a different ward going across a ward boundry to a different school and therefore impacting its scores. SO, the GCSE scores might actually be related to another spatial unit/ward.

  Plotting the residuals gives a clue to this. If similar coloured spatial units are near one another.

2. The solutions is to **incorporate a spatially-lagged version of this variable amongst the independent variables on the right-hand side of the equation.**
3. In the equation: PWiYi means if neighbours have higher values they might have more affect on others (aka: average value for GCSE scores is expected to be higher if, on average, Wards neighbouring also have higher GCSE score values), and here the **p represents spatial lag**
4. **you run a spatial lag regression model using a specific weight matrix**
  Using the queens case (see the code below)
  Using K nearest neighbours (see code below)
5. Note on coefficients:  model is not consistent bc observations change based on the weight matrix neighbours selected which might vary (almost certainly in a distance based matrix) SO we don't interpret the results exactly in the same way as with normal linear regression coefficients. **The only way to do this is through calculating the impact of the spatial lag!** which is impacts(model)
6. In order to compare the coefficients of the spatial lag model, you need to calculate the **"impact of the lag"** (code below) that can be determined in a variety of ways. There are direct and indirect components of this.


#Q: why are you using a global standardisation instead of a row standardisation in slag_dv_model2_queen? **You can use "W" which is row, the numbers will probs just be bigger**
#Q: can I just start with the lagrange multiplier test to tell if there is spatial autocorrelation? What does it mean for lm test and the wald test results to "be significant"? **Significance based on P value for the coefficient**

NEW Q: What is the "direct" and "indirect"?
  A: direct is like linear regression, the indirect is the influence of the surrounding variables. useful if you cant compare the coefficients, **Indirect is the impact of the lag on the coefficient**
NEW Q: From the KNN when you tried to fix the spatial autocorrelation, how do you read that chart to know if you were successful? What does each column really mean? The Rho has a rlly small P value. Is that what determines it? Would that not mean the lag still is there and is signif?
NEW Q: Is there ever NO spatial autocorrelation? If we run a OLS then moran's I and all seems fine, what can we conclude? Or maybe the spatial autocorrelation has a low p value... (p-value for queen's case here is 0.49704 which seems pretty high)
  A: **you could just skip lag and error, then do GWR and if its similar all across space then you can conclude linear regression is the best**
NEW Q: What is the desired result from running these models? What can it actually tell you about the regression you are trying to run? Like what is the so what conclusion you make at the end?
  A: tells you how good the model is, like if things are related

```{r}
model2 <- lm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014), data =LondonWardProfiles)

#Queens Case
slag_dv_model2_queen <- lagsarlm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014), data =LondonWardProfiles, nb2listw(LWard_nb, style="C"), method = "eigen") #I CAN USE ROW

t<-summary(slag_dv_model2_queen)

lrtest(slag_dv_model2_queen, model2)

#what do the outputs show?
tidy(slag_dv_model2_queen)
#glance(slag_dv_model2_queen)

#calculate the impact of the spatial lag
weight_list <- nb2listw(LWard_knn, style="C")
imp <- impacts(slag_dv_model2_queen, listw=weight_list)

##BEYOND THE SCOPE OF THE MODULE/WHAT WE REALLY NEED TO KNOW. This is used if you have a large data set and wish to do compute the direct and indirect.

#calculating the lag impact by row, "W"
slag_dv_model2_queen_row <- lagsarlm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 +log(median_house_price_2014), data = LondonWardProfiles, nb2listw(LWard_nb, style="W"), method = "eigen")

#This uses a sparse matrix. It takes two arguments: mult and MC
W <- as(weight_list, "CsparseMatrix")
trMatc <- trW(W, type="mult")
trMC <- trW(W, type="MC")
imp2 <- impacts(slag_dv_model2_queen_row, tr=trMatc, R=200)
imp3 <- impacts(slag_dv_model2_queen_row, tr=trMC, R=200)
sum <- summary(imp2,  zstats=TRUE, short=TRUE)

#K Nearest Neighbours
slag_dv_model2_knn4 <- lagsarlm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014), data = LondonWardProfiles, nb2listw(LWard_knn,style="C"),method = "eigen")

tidy(slag_dv_model2_knn4)

#Now we can check the residuals to make sure that the lag has been accounted for
LondonWardProfiles <- LondonWardProfiles %>% 
  mutate(slag_dv_model2_knn_resids = residuals(slag_dv_model2_knn4))

KNN4Moran <- LondonWardProfiles %>% 
  st_drop_geometry() %>% 
  dplyr::select(slag_dv_model2_knn_resids) %>% 
  pull() %>% 
  moran.test(., Lward.knn_4_weight) %>% 
  tidy()

KNN4Moran

```

## Spatial Error

Treats spatial errors as something to "deal with," a problem. **The error model deals with spatial autocorrelation (closer spatial units have similar values) of the residuals (vertical distance between your point and line of model – errors – over-predictions or under-predictions) again, potentially revealed though a Moran’s I analysis.The error model is not assuming that neighbouring independent variables are influencing the dependent variable but rather the assumption is of an issue with the specification of the model or the data used**

The SEM here suggests that spatially correlated errors in residuals leads to an overestimate of the importance of unauthorised absences in the OLS and an underestimation of the impact of affluence (aka median house prices). Here, the values for the variables are higher than in the SLM. In real life, what the model tells us here is that GCSE scores may be similar in bordering neighbourhoods *but not because students attend the same school but because students in these neighbouring places come from similar socio-economic or cultural backgrounds and this was not included as an independent variable in the original model.*

Here, **we can compare to OLS because there is no spatial lag anymore**

**Importantly, in the SLM and SEM, the p (not the normal p value, the one in the equ) and lambda values respectively are higher than their standard errors SO we can conclude that spatial dependence should be kept in mind when running the regression model**

We can also run the Lagrage Multiplier test here, a "robust" test, when you're not sure what is missing. This requires row standardisation in the spatial weights matrix. You can use this with both SLM and SEM. The code is just slightly different. LMerr is the spatial error model test and LMlag is the lagged test


Q: Ok, so it's a nuisance, what does that mean it does really? Does it remove values where there might be spatial autocorrelation?) Does it remove the spatial lag?
Q: How do we read the chart to even know it suggests this? Is it JUST the really low p-value for lambda? Is it because of the intercept?
Q: what exactly is a standard error and why does it matter?
  A: Like standard deviation, we want a lower standard error; 2x the standard error is like the normal dist. If its higher then you have something goinbg on
Q: if you use the SEM to determine when there might be a variable missing, how can it tell you what that variable is? And how do you know just from looking at the chart that it is missing?
Q: can you go over how to read the lagrange multiplier test? Is what I wrote about it above correct? What are RLMerr, RLMLag, and SARMA?

```{r}
sem_model1 <- errorsarlm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014), data = LondonWardProfiles, nb2listw(LWard_knn, style="C"), method = "eigen")

tidy(sem_model1)

Lward.queens_weight_ROW <- LWard_nb %>%
  nb2listw(., style="W")

lm.LMtests(model2, Lward.queens_weight_ROW, test = c("LMerr","LMlag","RLMerr","RLMlag","SARMA"))

```

## Geographically Weighted Regression: Part 1

GWR assumes that spatial autocorrelation is not a problem, but a global regression model of all our data doesn’t have the same regression slope - e.g. in certain areas (Boroughs, Wards) the relationship is different, termed non-stationary. GWR runs a local regression model for adjoining spatial units and shows how the coefficients can vary over the study area.

Dummy Variables: always categorical data (inner or outer London, or red / blue etc.) They split our analysis into groups with each one having a seperate regression line. This entails writing code to turn the variables into "factors" (a class)
1. Below, the dummy variable is statistically significant (a low p value) and so this tells us that the difference between the two groups (Inner and Outer London) we are comparing. In this case, it is telling us that living in a Ward in outer London will improve your average GCSE score by 10.93 points, on average, compared to if you lived in Inner London.
2. The order in which the dummy comparisons are made is determined by what is known as a ‘contrast matrix’. This determines the treatment group (1) and the control (reference) group (0). In the model, you only get one coefficient for the dummy variables. One of the dummy variables is always the "reference" varaible which we can change using the contrast() or reference function() (see code below).

Q: why would we want to change the reference variable when we use dummy variables?


```{r}
extradata <- read_csv("https://www.dropbox.com/s/qay9q1jwpffxcqj/LondonAdditionalDataFixed.csv?raw=1")

#add the extra data too
LondonWardProfiles <- LondonWardProfiles%>%
  left_join(., extradata, by = c("gss_code" = "Wardcode"))%>%
  clean_names()

#print some of the column names
LondonWardProfiles%>%
  names()%>%
  tail(., n=10)

p <- ggplot(LondonWardProfiles, aes(x=unauth_absence_schools11, y=average_gcse_capped_point_scores_2014))
p + geom_point(aes(colour = inner_outer))

#first, let's make sure R is reading our InnerOuter variable as a factor
#see what it is at the moment...
isitfactor <- LondonWardProfiles %>%
  dplyr::select(inner_outer)%>%
  summarise_all(class)

# change the categorical variables to the class "factor"
LondonWardProfiles<- LondonWardProfiles %>%
  mutate(inner_outer=as.factor(inner_outer))

#now run the model
model3 <- lm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014) + inner_outer, data = LondonWardProfiles)
 
# changing the reference variable using the contrast() or relevel() function
LondonWardProfiles <- LondonWardProfiles %>%
  mutate(inner_outer = relevel(inner_outer, ref="Outer"))

model3 <- lm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014) + inner_outer, data = LondonWardProfiles)

tidy(model3)

```

## Spatial Non-stationarity & Geographically Weighted Regression: Part 2
1. One of the biggest issues with GWR is that it can be over or underfit for the model.
2. Run the regression model, check the residuals, check using moran's I for spatial autocorrelation
3. We might have **'non-stationarity’ - this is when the global model does not represent the relationships between variables that might vary locally.**
4. One of the **key things you look for is the bandwidth.** It refers to the distance beyond which a value of zero is assigned to weight observations. Larger bandwidths include a larger number of observations receiving a non-zero weight and more observations are used to fit a local regression. 
5. The main package for this is spgwr
6. In the GWR code, adapt=T here means to automatically find the proportion of observations for the weighting using k nearest neighbours (an adaptive bandwidth), False would mean a global bandwidth and that would be in meters (as our data is projected)
**The optimal bandwidth is about 0.015 meaning 1.5% of all the total spatial units should be used for the local regression based on k-nearest neighbours.**
7. In the code for the GWR:
  a) you can also take a longitude and latitude column directly from a csv. You would add: longlat=TRUE and then the columns selected in the coords argument e.g. coords=cbind(long, lat)
  b) this code searches for the optimal bandwidth using "cross validation." The specific model gwr.sel() which uses the default gaussian weighting scheme where near points are given more value/influence increases with distance.
  c) we could also change the argument and use gweight = gwr.Gauss in the gwr.sel() function. gwr.bisquare() is the other option
  d) we can also set the number of neighbours, you would do specific_number/total, just change the adapt value to the number 
  e) you can also replace adapt with bandwidth and specify that specifically
  
8. To compare GWR models, you should use the AIC (which is included in the output below). It comes from the number of independent variables and maximum likelihood estimate of the model (how well the model reproduces the data. **The lower the value the better the better the model fit is**
9. Once you get your results, you can run a significance test. **If this is greater than zero (i.e. the estimate is more than two standard errors away from zero), it is very unlikely that the true value is zero, i.e. it is statistically significant (at nearly the 95% confidence level)**

```{r}
#select some variables from the data file
myvars <- LondonWardProfiles %>%
  dplyr::select(average_gcse_capped_point_scores_2014, unauthorised_absence_in_all_schools_percent_2013, median_house_price_2014, rate_of_job_seekers_allowance_jsa_claimants_2015, percent_with_level_4_qualifications_and_above_2011, inner_outer)

#check their correlations are OK
Correlation_myvars <- myvars %>%
  st_drop_geometry()%>%
  dplyr::select(-inner_outer)%>%
  correlate()

#run a final OLS model
model_final <- lm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014) + inner_outer + rate_of_job_seekers_allowance_jsa_claimants_2015 +  percent_with_level_4_qualifications_and_above_2011, data = myvars)

tidy(model_final)

LondonWardProfiles <- LondonWardProfiles %>%
  mutate(model_final_res = residuals(model_final))

par(mfrow=c(2,2))
plot(model_final)

qtm(LondonWardProfiles, fill = "model_final_res")

#Now, run a moran test on the residuals to check for spatial autocorrelation
final_model_Moran <- LondonWardProfiles %>%
  st_drop_geometry()%>%
  dplyr::select(model_final_res)%>%
  pull()%>%
  moran.test(., Lward.knn_4_weight)%>%
  tidy()

#Find the GWR bandwith <- see the comments above!
coordsW2 <- st_coordinates(coordsW)

LondonWardProfiles2 <- cbind(LondonWardProfiles,coordsW2)

GWRbandwidth <- gwr.sel(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + log(median_house_price_2014) + inner_outer + rate_of_job_seekers_allowance_jsa_claimants_2015 + percent_with_level_4_qualifications_and_above_2011, data = LondonWardProfiles2, coords=cbind(LondonWardProfiles2$X, LondonWardProfiles2$Y), adapt=T)

gwr.model = gwr(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + 
                    log(median_house_price_2014) + 
                    inner_outer + 
                    rate_of_job_seekers_allowance_jsa_claimants_2015 +
                    percent_with_level_4_qualifications_and_above_2011, 
                  data = LondonWardProfiles2, 
                coords=cbind(LondonWardProfiles2$X, LondonWardProfiles2$Y), 
                adapt=GWRbandwidth,
                #matrix output
                hatmatrix=TRUE,
                #standard error
                se.fit=TRUE)

results <- as.data.frame(gwr.model$SDF)
names(results)

#attach coefficients to original SF
LondonWardProfiles2 <- LondonWardProfiles %>%
  mutate(coefUnauthAbs = results$unauthorised_absence_in_all_schools_percent_2013,
         coefHousePrice = results$log.median_house_price_2014.,
         coefJSA = rate_of_job_seekers_allowance_jsa_claimants_2015,
         coefLev4Qual = percent_with_level_4_qualifications_and_above_2011)

tm_shape(LondonWardProfiles2) +
  tm_polygons(col = "coefUnauthAbs", 
              palette = "RdBu", 
              alpha = 1)
tm_shape(LondonWardProfiles2) +
  tm_polygons(col = "coefHousePrice", 
              palette = "RdBu", 
              alpha = 1)

#run the significance test:
sigTest = abs(gwr.model$SDF$"log(median_house_price_2014)")-2 * gwr.model$SDF$"log(median_house_price_2014)_se"

#store significance results
LondonWardProfiles2 <- LondonWardProfiles2 %>%
  mutate(GWRUnauthSig = sigTest)

#map the significance
tm_shape(LondonWardProfiles2) +
  tm_polygons(col = "GWRUnauthSig", 
              palette = "RdYlBu")
```

Based on the above analysis, when there is a negative coefficient, GCSE scores go down and positive when they go up. So, for the unauthorised absences map, in wards where the coefficient is negative, when unauthorised absences increase, scores decrease by the ammount of the coefficient. **Of course, these results may not be statistically significant across the whole of London. Roughly speaking, if a coefficient estimate is more than 2 standard errors away from zero, then it is “statistically significant”.**

